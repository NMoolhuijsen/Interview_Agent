<<<<<<< HEAD:tutorials/RAG_tutorial/rag.ipynb
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install langchain-openai\n",
    "! pip install beautifulsoup4 # scraper\n",
    "! pip install faiss-cpu      # vector storage and retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"sk-<your-api-key>\"\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import HuggingFaceEndpoint\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain, create_history_aware_retriever\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example with OpenAI LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI()\n",
    "llm.invoke(\"what is docker and how is it useful for deployment\") # return the response from the model\n",
    "\n",
    "# creating a prompt template\n",
    "prompt = ChatPromptTemplate.from_messages([ \n",
    "    (\"system\", \"You are an English-French translator that returns whatever the user says in French.\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "]) \n",
    "\n",
    "# create chain using prompt\n",
    "chain = prompt | llm\n",
    "chain.invoke({ \n",
    "    \"input\": \"i enjoy going to rock concerts\" # var: input. Can be multiple.\n",
    "    }) \n",
    "\n",
    "# NOTE output is AIMessage(content=\"j'aime aller aux concerts de rock\")\n",
    "\n",
    "# add output parser: defines output structure\n",
    "output_parser = StrOutputParser()\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "chain.invoke({\"input\": \"my friend robert had a blue cat\"}) # return the response from the model\n",
    "# NOTE output now is \"mon ami robert a un chat bleu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize webpage loader\n",
    "loader = WebBaseLoader(\"url\") # url of webpage you want to use\n",
    "docs = loader.load() # load the documents from the webpage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question-asking from documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding model to add info to vector store\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# create vector store from documents\n",
    "text_splitter = RecursiveCharacterTextSplitter()\n",
    "documents = text_splitter.split(docs) # split the documents into smaller parts\n",
    "vectorstore = FAISS.from_documents(documents, embeddings) # create vector store from documents\n",
    "\n",
    "# create prompt template for retrieval chain\n",
    "template = \"\"\" Answer the following quesion based only on the provided context:\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {input}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# create document chain from llm and prompt\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "document_chain.invoke({\n",
    "    \"input\": \"What is langchain 0.1.0?\",\n",
    "    \"context\": [Document(page_content=\"langchain 0.1.0 is a new version of langchain\")]\n",
    "})\n",
    "# NOTE context is hardcoded. No usage of documents yet. \n",
    "\n",
    "# create retrieval chain WITH documents\n",
    "retriever = vectorstore.as_retriever()\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
    "# NOTE retrieval chain retrieves most important documents based on the input\n",
    "\n",
    "# get reponse from retrieval chain\n",
    "response = retrieval_chain.invoke({\n",
    "    \"input\": \"What is new in langchain 0.1.0?\"\n",
    "})\n",
    "# returns repsonse from the model including input, context and answer\n",
    "\n",
    "response['answer'] # returns the answer only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversation with memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conversational retrieval chain; uses previous context\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"), # use history if it exists\n",
    "    (\"user\", \"{input}\"), \n",
    "    (\"user\", \"Given the above conversation, generate a search query to get above information\")\n",
    "])\n",
    "retriever_chain = create_history_aware_retriever(llm, retriever, prompt)\n",
    "\n",
    "# create mock history conversation\n",
    "chat_history = [\n",
    "    HumanMessage(content=\"Is there anything new about langchain 0.1.0?\"),\n",
    "    AIMessage(content=\"yes.\")\n",
    "]\n",
    "\n",
    "retriever_chain.invoke({\n",
    "    \"chat_history\": chat_history,\n",
    "    \"input\": \"Tell me more about it.\"\n",
    "})\n",
    "\n",
    "# NOTE this returns multiple snippets matching the context, metadata, the input etc.\n",
    "\n",
    "# return only the answer\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Answer the questions based on the below context:\\n\\n{context}\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "conversational_retrieval_chain = create_retrieval_chain(retriever_chain, document_chain)\n",
    "\n",
    "# get response based on earlier conversation\n",
    "response = conversational_retrieval_chain.invoke({\n",
    "    \"chat_history\": chat_history,\n",
    "    \"input\": \"Tell me more about it?\"\n",
    "})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
=======
import streamlit as st
from dotenv import load_dotenv
from PyPDF2 import PdfReader
from langchain.text_splitter import CharacterTextSplitter
from langchain_community.embeddings import OpenAIEmbeddings, HuggingFaceInstructEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.chat_models import ChatOpenAI, ChatHuggingFace
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
from htmlTemplate import bot_template, user_template, css
from langchain_community.llms import HuggingFaceEndpoint

def get_pdf_text(pdf_docs):
    text = ""	# empty string
    for pdf in pdf_docs:	# iterate through pdfs
        pdf_reader = PdfReader(pdf)	# read pdf
        for page in pdf_reader.pages:	# iterate through pages
            text += page.extract_text()	# extract text from page
    
    return text	# return raw text

def get_text_chunks(text):
    text_splitter = CharacterTextSplitter(	# create text splitter
    separator="\n",
    chunk_size=1000,
    chunk_overlap=200,
    length_function=len,
    )
    chunks = text_splitter.split_text(text)	# split text into chunks
    return chunks	# return text chunks

def get_vectorstore(text_chunks):
    """
    Create vector store from text chunks. Can be done via OpenAI or HuggingFaceInstructEmbeddings
    This variant uses model Instructor from HuggingFace.
    """	
    # embeddings = OpenAIEmbeddings()	# create embeddings in OpenAI
    embeddings = HuggingFaceInstructEmbeddings(model_name="hkunlp/instructor-xl")	# create embeddings in HuggingFace
    vectorstore = FAISS.from_texts(texts=text_chunks, embedding=embeddings)    # create vector store
    return vectorstore	# return vector store

def get_conversation_chain(vectorstore):
    # llm = ChatHuggingFace()	# choose llm
    llm = HuggingFaceEndpoint(repo_id="google/gemma-7b") # choose llm from huggingface
    memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)	# create conversation chain
    conversation_chain = ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=vectorstore.as_retriever(),
        memory=memory,
    )
    return conversation_chain	# return conversation chain

def handle_userinput(user_question):
    response = st.session_state.conversation({'question': user_question})	# get response
    st.session_state.chat_history = response['chat_history']	# save chat history
    # print(st.session_state.chat_history)

    for i, message in enumerate(st.session_state.chat_history):	# iterate through chat history
        # show user message
        if i % 2 == 0:
            st.write(user_template.replace("{{MSG}}", message.content), unsafe_allow_html=True)	
        # show bot message
        else:
            st.write(bot_template.replace("{{MSG}}", message.content), unsafe_allow_html=True)	


def main():
    load_dotenv() # load environment variables
    st.set_page_config(page_title="Chat with multiple PDFs", page_icon=":books:") # user interface
    st.write(css, unsafe_allow_html=True) # write css

    if "conversation" not in st.session_state: # if conversation is not in session state	
        st.session_state.conversation = None # set conversation to None. Else, use saved conversation
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = None

    st.header("Chat with multiple PDFs :books:")
    user_question = st.text_input("Ask a question about your documents:") # label for user input, hardcoded conv
    if user_question: # if user question is not empty
        handle_userinput(user_question) # handle user input

    st.write(user_template.replace("{{MSG}}", "hello bot"), unsafe_allow_html=True) # replace user template with hello bot
    st.write(bot_template.replace("{{MSG}}", "hello human"), unsafe_allow_html=True) 

    with st.sidebar:
        st.subheader("Upload PDFs")
        pdf_docs = st.file_uploader("Upload a PDF file", type="pdf", accept_multiple_files=True) # file uploader
        if st.button( "Upload"): # button to upload file
            st.spinner("Process") #
            with st.spinner("Processing"):	 # show processing message while processing
                # get pdf text
                raw_text = get_pdf_text(pdf_docs) # single string of text
                # st.write(raw_text) # display text
                # get text chunks
                text_chunks = get_text_chunks(raw_text) # list of text chunks
                # st.write(text_chunks)
                # create vector store
                vectorstore = get_vectorstore(text_chunks)

                # create conversation chain
                # use to keep the variable from being reset/reinitialized
                st.session_state.conversation = get_conversation_chain(vectorstore) 
                # conversation = get_conversation_chain(vectorstore) 


if __name__ == '__main__':
    main()

>>>>>>> 78e0bfc49d1256ce9051c7a2e13d6c1989364f65:RAG_tutorial/rag.ipynb
