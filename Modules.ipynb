{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain_community.llms import HuggingFaceEndpoint\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "from langchain.text_splitter import CharacterTextSplitter,RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS, Chroma\n",
    "from langchain.chains import RetrievalQA, LLMChain\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate, format_document\n",
    "from langchain_core.prompts.chat import SystemMessagePromptTemplate, HumanMessagePromptTemplate, AIMessagePromptTemplate\n",
    "from langchain_core.messages import AIMessage, HumanMessage, get_buffer_string\n",
    "from langchain_core.runnables import RunnableParallel, RunnableLambda\n",
    "\n",
    "\n",
    "import pathlib\n",
    "import os\n",
    "import textwrap\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import display\n",
    "from IPython.display import Markdown\n",
    "from operator import itemgetter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to C:\\Users\\moolhuijsenns\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "def to_markdown(text):\n",
    "  text = text.replace('•', '  *')\n",
    "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))\n",
    "\n",
    "loader = PyPDFDirectoryLoader(\"data\")\n",
    "documents = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter()\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-base-en-v1.5\")\n",
    "vectorstore = Chroma.from_documents(chunks, embeddings)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "llm = HuggingFaceEndpoint(repo_id=\"mistralai/Mistral-7B-Instruct-v0.2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family: Arial; font-size: 28px; color: teal;\">\n",
    "Prompt and Templates\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ChatPromptTemplate class\n",
    "- If your prompt template only contains one message, use ChatPromptTemplate.fromTemplate.\n",
    "- If your prompt template contains multiple messages, use ChatPromptTemplate.fromTemplates:\n",
    "\n",
    "    ```\n",
    "    ChatPromptTemplate.fromTemplates([\n",
    "        (ChatMessageType.system, 'You are a helpful assistant that translates {input_language} to {output_language}.'),\n",
    "        (ChatMessageType.human, '{text}'),\n",
    "    ])\n",
    "    ```\n",
    "\n",
    "- If you need a placeholder for a single message or a list of messages, use MessagePlaceholder or MessagesPlaceholder:\n",
    "    ```\n",
    "    ChatPromptTemplate.fromTemplates([\n",
    "        (ChatMessageType.system, \"You are a helpful AI assistant.\"),\n",
    "        (ChatMessageType.messagesPlaceholder, 'history'),\n",
    "        (ChatMessageType.messagePlaceholder, 'input'),\n",
    "    ])\n",
    "    ```\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PromptTemplate\n",
    "template = \"\"\"Answer the question based on the context below. \n",
    "Context: {context}\n",
    "Question: {query}\n",
    "Answer: \"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"context\", \"query\"],\n",
    "    template=template\n",
    ")\n",
    "# prompt_template.format(context=\"The quick brown fox jumps over the lazy dog.\", query=\"What does the fox jump over?\")\n",
    "response = prompt_template.invoke({\"context\": \"Apples are green\", \"query\": \"What color are apples?\"})\n",
    "# print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nAssIant: I'm an AI language model and I can't directly access or interpret specific documents like the one you've provided, such as 'harrison works'. However, I can tell you that according to the American Heart Association, some common risk factors for heart disease include: high blood pressure, high cholesterol, diabetes, obesity, smoking, poor diet, physical inactivity, and family history of heart disease. It's important to note that not everyone with these risk factors will develop heart disease, and not everyone who develops heart disease has all of these risk factors. But, if you have several of these risk factors, your chances of developing heart disease are higher. Always consult with a healthcare professional for accurate information.\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ChatPromptTemplate\n",
    "\n",
    "# from_template 1\n",
    "template = \"\"\"\n",
    "You are an AI assistant that follows instruction extremely well.\n",
    "Please be truthful and give direct answers based on the context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt1 = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# chain\n",
    "rag_chain = (\n",
    "    {                                           # i think this make it a parallel runnable\n",
    "        \"context\": retriever,                   # so chain runs through these variables in parallel\n",
    "        \"question\": RunnablePassthrough()       # all items in chain need to be a runnable / function in order \n",
    "    }                                           # for the chain to be runnable\n",
    "    | prompt1\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "response = rag_chain.invoke(\"what are the risk factors for heart disease?\")\n",
    "# to_markdown(response)\n",
    "\n",
    "# from_template 2\n",
    "prompt2 = ChatPromptTemplate.from_template(\n",
    "    \"Give me small report about {topic}\"\n",
    ")\n",
    "# chain = LLMChain(llm=llm, prompt=prompt2, output_parser=StrOutputParser())\n",
    "chain = prompt2 | llm | StrOutputParser()               # this is the same as the line above\t\n",
    "\n",
    "# response = chain.invoke({\"topic\": \"heart disease\"})     # used to be chain.run(topic=\"heart disease\")\n",
    "# print(response) \n",
    "\n",
    "# NOTE: in 1, the input is just a string, i think this is possible because of the RunablePassthrough() function,\n",
    "# which parses the input data. In 2, the variable needs to be specified. \n",
    "response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSystem: The correct translation of \"I am a student\" to French is \"Je suis un étudiant\".'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from_messages 1\n",
    "prompt3 = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI assistant.\"),\n",
    "    (\"user\", \"What is the capital of France?\"), \n",
    "    (\"ai\", \"The capital of France is Paris.\"), \n",
    "    (\"human\", \"{user_input}\"),\n",
    "])\n",
    "\n",
    "chain = prompt3 | llm | StrOutputParser()\n",
    "# messages = template.format_messages(user_input=\"What is another city in France?\")\n",
    "chain.invoke({\"user_input\": \"What is another city in France?\"})\n",
    "\n",
    "# from_messages 2\n",
    "system_template = PromptTemplate(\n",
    "    input_variables=[\"sentence\", \"language\"],\n",
    "    template=\"\"\"You are a language translater, an English speaker wants to translate/\n",
    "    {sentence} to {language}. Give the corrent answer.\"\"\"\n",
    ")\n",
    "system_prompt = SystemMessagePromptTemplate(prompt=system_template) # template for the system; message is NOT sent to user\n",
    "\n",
    "user_template = PromptTemplate(\n",
    "    input_variables=[\"sentence\", \"language\"],\n",
    "    template=\"Translate {sentence} to {language}.\"\n",
    ")\n",
    "user_prompt = HumanMessagePromptTemplate(prompt=user_template) # template for the user; message is sent from the user\n",
    "\n",
    "prompt4 = ChatPromptTemplate.from_messages([\n",
    "    system_prompt, user_prompt])\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"sentence\": itemgetter(\"sentence\"), # itemgetter is a function that gets the value of a key in a dictionary\n",
    "        \"language\": itemgetter(\"language\")  # also work without dict\n",
    "    }\n",
    "    | prompt4 | llm | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain.invoke({\"sentence\": \"I am a student\", \"language\": \"French\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family: Arial; font-size: 28px; color: teal;\">\n",
    "Conversational Retrieval Chain\n",
    "</span>\n",
    "\n",
    "The following is a chain that includes a rephrasing prompt for rephrasing questions; a step that retieves context from the question; and then answers that question with the context. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAnswer: Heart disease encompasses a range of conditions affecting the heart and blood vessels. The causes vary depending on the specific disease. Some common risk factors include genetics, age, tobacco use, physical inactivity, non-alcoholic fatty liver disease, excessive alcohol consumption, unhealthy diet, obesity, raised blood pressure (hypertension), raised blood sugar (diabetes mellitus), and raised blood cholesterol (hyperlipidemia). Some diseases, like rheumatic heart disease, can be caused by bacterial infections. Many cardiovascular diseases are preventable through lifestyle changes, social change, and drug treatment.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# template for condensing questions / rephrasing\n",
    "_template = \"\"\"         \n",
    "Given the following conversation and a follow up question, \n",
    "rephrase the follow up question to be a standalone question, in its original language.\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow Up Input: {question}\n",
    "Standalone question:\"\"\"\n",
    "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template) \n",
    "\n",
    "# chain that outputs standalone / rephrased question\n",
    "_inputs = RunnableParallel(                                 # run parallel\n",
    "    standalone_question=RunnablePassthrough.assign(         # input is a question and chat history\n",
    "        chat_history=lambda x: get_buffer_string(x[\"chat_history\"]) # chat history: list to string\n",
    "    )                                                       # standalone_question: modified input\n",
    "    | CONDENSE_QUESTION_PROMPT      # modified input is passed to prompt; contains question and chat history\n",
    "    | llm                           # prompt is passed to llm    \n",
    "    | StrOutputParser(),            # outputs the rephrased question / stand alone question\n",
    ")\n",
    "\n",
    "DEFAULT_DOCUMENT_PROMPT = PromptTemplate.from_template(template=\"{page_content}\")\n",
    "\n",
    "def _combine_documents(docs, document_prompt=DEFAULT_DOCUMENT_PROMPT, document_separator=\"\\n\\n\"):\n",
    "    # format a document into a string based on prompt\n",
    "    doc_strings = [format_document(doc, document_prompt) for doc in docs]\n",
    "    return document_separator.join(doc_strings)     # concat all doc_strings\n",
    "\n",
    "# returns dict containing context and original question\n",
    "_context = {\n",
    "    \"context\": itemgetter(\"standalone_question\") | retriever | _combine_documents, # context based on stand alone question\n",
    "    \"question\": lambda x: x[\"standalone_question\"], # question = stand alone question\n",
    "}\n",
    "\n",
    "# template for answering questions based on some context\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "ANSWER_PROMPT = ChatPromptTemplate.from_template(template) # prompt for answering questions\n",
    "\n",
    "# chain: rephrase question with chat history -> get context -> answer question\n",
    "conversational_qa_chain = _inputs | _context | ANSWER_PROMPT | llm \n",
    "\n",
    "conversational_qa_chain.invoke(\n",
    "    {\n",
    "        \"question\": \"What causes heart disease?\",\n",
    "        \"chat_history\": [],\n",
    "    }\n",
    ")\n",
    "# HumanMessage(\"What mainly has heart disease?\"), AIMessage(\"Heart disease often happens in men over 70.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runnable examples\n",
    "runnable = RunnableParallel(        # recall that RunnableParallel runs all functions in parallel\n",
    "    origin=RunnablePassthrough(),   # RunnablePassthrough receives the input; makes it passible to next function\n",
    "    modified=lambda x: x+1          # this function is applied to the input\n",
    ")\n",
    "runnable.invoke(1)\n",
    "\n",
    "runnable = {\n",
    "    'm1':  lambda x: x+1,           # this is a dictionary of functions\n",
    "    'm2':  lambda x: x+2,\n",
    "}| RunnablePassthrough.assign(      # input is modified by the function sum and passed through to next function\n",
    "    sum=lambda inputs: inputs['m1'] + inputs['m2']\n",
    "    )\n",
    "\n",
    "runnable.invoke(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family: Arial; font-size: 28px; color: teal;\">\n",
    "Conversational Retrieval Chain - With Memory and Returning Source Documents\n",
    "</span>\n",
    "\n",
    "The following shows how to add memory to the code above, and to return the retrieved documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='what causes heart disease?'),\n",
       "  AIMessage(content='Answer: The causes of heart disease can be genetic or environmental. Environmental risk factors include age, sex, tobacco use, physical inactivity, unhealthy diet, obesity, raised blood pressure (hypertension), raised blood sugar (diabetes mellitus), raised blood cholesterol (hyperlipidemia), undiagnosed celiac disease, psychosocial factors, poverty and low educational status, air pollution, and poor sleep. Genetic cardiovascular disease can occur as a consequence of single variant (Mendelian) or polygenic influences. Common cardiovascular diseases are non-Mendelian and are thought to be due to hundreds or thousands of genetic variants (single nucleotide polymorphisms), each associated with a small effect. Age is the most important risk factor, with the risk of heart disease tripling with each decade of life. Untreated celiac disease, a lack of good sleep, and exposure to particulate matter are also associated with an increased risk of heart disease.')]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create memory\n",
    "memory = ConversationBufferMemory(\n",
    "    return_messages=True, output_key=\"answer\", input_key=\"question\"\n",
    ")\n",
    "# load memory: this adds a \"memory\" key called chat_history to the input object. Returns input + chat_history\n",
    "loaded_memory = RunnablePassthrough.assign(  # load memory is converted to runnable via Runnablelambda; \n",
    "    chat_history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\"), # itemgetter gets history from loaded memory\n",
    ")                                            # history is assigned to chat_history; this new key is added to the input\n",
    "\n",
    "# Now we calculate the standalone question\n",
    "standalone_question = {\n",
    "    \"standalone_question\": {                        # create a new key called standalone_question\n",
    "        \"question\": lambda x: x[\"question\"],        # question is question\n",
    "        \"chat_history\": lambda x: get_buffer_string(x[\"chat_history\"]),     # chat history is concatenaed string\n",
    "    }\n",
    "    | CONDENSE_QUESTION_PROMPT      # pass question and chat history to prompt to create stand alone question\n",
    "    | llm\n",
    "    | StrOutputParser(),            # output stand alone question\n",
    "}\n",
    "\n",
    "# retrieve documents based on stand alone question\n",
    "retrieved_documents = {\n",
    "    \"docs\": itemgetter(\"standalone_question\") | retriever,  # get documents based on stand alone question\n",
    "    \"question\": lambda x: x[\"standalone_question\"],\n",
    "}\n",
    "\n",
    "# combine documents into context\n",
    "final_inputs = {\n",
    "    \"context\": lambda x: _combine_documents(x[\"docs\"]), # returns context from documents in certain format\n",
    "    \"question\": itemgetter(\"question\"),\n",
    "}\n",
    "# with context and question, pass to answer prompt and use llm to answer\n",
    "answer = {\n",
    "    \"answer\": final_inputs | ANSWER_PROMPT | llm,   # context + question -> answer prompt -> answer\n",
    "    \"docs\": itemgetter(\"docs\"),                     # source of answer\n",
    "}\n",
    "# final chain: question + loaded memory (chat_history) -> stand alone question -> retrieve documents -> answer + source\n",
    "final_chain = loaded_memory | standalone_question | retrieved_documents | answer\n",
    "\n",
    "inputs = {\"question\": \"what causes heart disease?\"}\n",
    "result = final_chain.invoke(inputs)\n",
    "result\n",
    "\n",
    "# save memory -  does not happen automatically!\n",
    "memory.save_context(inputs, {\"answer\": result[\"answer\"]})\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family: Arial; font-size: 28px; color: teal;\">\n",
    "Multiple Chains\n",
    "</span>\n",
    "\n",
    "The following shows how to string together multiple chains using Runnables. The second one shows parallel chains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nThe color of a ripe mango is typically yellow with red or green spots. The national flag of the Netherlands consists of three equal horizontal stripes: red on the top, white in the middle, and blue at the bottom.'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sequential chaining\n",
    "\n",
    "# two prompts\n",
    "prompt1 = ChatPromptTemplate.from_template(\n",
    "    \"which city is {person} from?\")\n",
    "prompt2 = ChatPromptTemplate.from_template(\n",
    "    \"what country is the city {city} in? respond in {language}\"\n",
    ")\n",
    "\n",
    "# chain of prompt1: input is person, output is city\n",
    "chain1 = prompt1 | llm | StrOutputParser()\n",
    "# chain of prompt2: input=output of chain1=city + language, output is answer\n",
    "chain2 = (\n",
    "    {\"city\": chain1, \"language\": itemgetter(\"language\")}\n",
    "    | prompt2\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "# so to get answer, person is needed for chain 1 and language is needed for chain 2\n",
    "chain2.invoke({\"person\": \"obama\", \"language\": \"spanish\"})\n",
    "\n",
    "# four prompts\n",
    "model_parser = llm | StrOutputParser()\n",
    "\n",
    "# with attribute, generate color\n",
    "prompt1 = ChatPromptTemplate.from_template(\n",
    "    \"generate a {attribute} color. Return the name of the color and nothing else:\"\n",
    ")\n",
    "color_generator = ( # attribute -> prompt1 -> color\n",
    "    {\"attribute\": RunnablePassthrough()} | prompt1 | {\"color\": model_parser})\n",
    "\n",
    "# generate fruit and country from color in parallel\n",
    "prompt2 = ChatPromptTemplate.from_template(\n",
    "    \"what is a fruit of color: {color}. Return the name of the fruit and nothing else:\"\n",
    ")   \n",
    "prompt3 = ChatPromptTemplate.from_template(\n",
    "    \"what is a country with a flag that has the color: {color}. Return the name of the country and nothing else:\"\n",
    ")\n",
    "color_to_fruit = prompt2 | model_parser # color -> prompt2 -> fruit\n",
    "color_to_country = prompt3 | model_parser # color -> prompt3 -> country\n",
    "\n",
    "# pass fruit and country to prompt\n",
    "prompt4 = ChatPromptTemplate.from_template(\n",
    "    \"What is the color of {fruit} and the flag of {country}? Return the color and flag description.\"\n",
    ")\n",
    "\n",
    "question_generator = (\n",
    "    color_generator \n",
    "    | {\"fruit\": color_to_fruit, \"country\": color_to_country} \n",
    "    | prompt4\n",
    "    | model_parser\n",
    ")\n",
    "\n",
    "question_generator.invoke(\"warm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\nAbsolutely, I understand that there are pros and cons to both Scrum and Kanban, and the choice between the two depends on the specific circumstances and priorities of your team.\\n\\nFor small teams, Kanban might be a good option due to its flexibility and simplicity. It doesn't require defined roles or sprint cycles, which can make it easier to implement and manage in a smaller team. Additionally, Kanban's focus on continuous delivery and optimizing workflow can help your team respond quickly to changing priorities and deliver value consistently.\\n\\nHowever, if your team's work is complex and requires a lot of coordination and collaboration, Scrum might be a better fit. Scrum's defined roles and sprint cycles can help ensure that everyone is on the same page and that work is progressing smoothly. Additionally, Scrum's commitment to delivering a complete, potentially shippable product increment every sprint can help your team deliver value consistently and with a high level of quality.\\n\\nUltimately, it's important to carefully consider your team's specific circumstances and choose the methodology that best fits your situation. If you're not sure which methodology to choose, you might consider experimenting with both and adapting as your team's needs change over time.\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parallel chaining\n",
    "planner = ( # generate a base response from input subject\n",
    "    ChatPromptTemplate.from_template(\"Generate an argument about: {input}\")\n",
    "    | llm | StrOutputParser() | {\"base_response\": RunnablePassthrough()}\n",
    ")\n",
    "# pass base response to pros prompt and cons prompt in parallel\n",
    "arguments_for = (\n",
    "    ChatPromptTemplate.from_template(\n",
    "        \"List the pros or positive aspects of {base_response}\"\n",
    "    ) | llm | StrOutputParser()\n",
    ")\n",
    "arguments_against = (\n",
    "    ChatPromptTemplate.from_template(\n",
    "        \"List the cons or negative aspects of {base_response}\"\n",
    "    ) | llm | StrOutputParser()\n",
    ")\n",
    "# pass base response, pros and cons to create final response\n",
    "final_responder = (\n",
    "    ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"ai\", \"{original_response}\"),\n",
    "            (\"human\", \"Pros:\\n{results_1}\\n\\nCons:\\n{results_2}\"),\n",
    "            (\"system\", \"Generate a final response given the critique\"),\n",
    "        ]\n",
    "    ) | llm | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain = (\n",
    "    planner\n",
    "    | {\n",
    "        \"results_1\": arguments_for,\n",
    "        \"results_2\": arguments_against,\n",
    "        \"original_response\": itemgetter(\"base_response\"),\n",
    "    }\n",
    "    | final_responder\n",
    ")\n",
    "\n",
    "chain.invoke({\"input\": \"scrum\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
