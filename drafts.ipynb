{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports \n",
    "from dotenv import load_dotenv\n",
    "import streamlit as st\n",
    "import os\n",
    "\n",
    "from langchain_community.vectorstores import VectorStore, FAISS, Chroma\n",
    "from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader, PyPDFDirectoryLoader\n",
    "from langchain_community.llms import HuggingFaceEndpoint\n",
    "\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.embeddings import GPT4AllEmbeddings\n",
    "\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain, create_history_aware_retriever\n",
    "from langchain.chains import QAGenerationChain, ConversationalRetrievalChain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter \n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder, PromptTemplate\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to C:\\Users\\moolhuijsenns\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "# retriever - llm\n",
    "def get_retriever(loader):\n",
    "    # get documents from data folder\n",
    "    documents = loader.load()\n",
    "\n",
    "    # split documents into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=10)\n",
    "    texts = text_splitter.split_documents(documents)\n",
    "\n",
    "    # create retriever\n",
    "    embeddings = GPT4AllEmbeddings()\n",
    "    vector_store = FAISS.from_documents(texts, embeddings)\n",
    "    retriever = vector_store.as_retriever()\n",
    "    return retriever\n",
    "\n",
    "loader = PyPDFDirectoryLoader('data')\n",
    "retriever = get_retriever(loader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to C:\\Users\\moolhuijsenns\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'statement': 'On the night of March 18th, 2024, around 8:30 PM, I saw a guy rob a girl outside the convenience store on Elm Street. He was tall, kinda skinny, looked like he was in his 30s, wearing a black hoodie and jeans with a backpack. The girl seemed young, maybe in her early 20s, wearing a red jacket and jeans with long blonde hair. So, this guy walks up to her, pulls out what looked like a knife, and tells her to hand over her purse. She looked scared, so she gave it to him, and he took off running down Elm Street.',\n",
       " 'what': ['A guy robbed a girl',\n",
       "  'The guy pulled out a knife',\n",
       "  'The girl gave the guy her purse'],\n",
       " 'when': ['March 18th, 2024', 'around 8:30 PM'],\n",
       " 'where': ['outside the convenience store', 'Elm Street'],\n",
       " 'who': ['a guy', 'the girl'],\n",
       " 'how': ['the guy pulled out a knife and told the girl to hand over her purse']}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import ListOutputParser, CommaSeparatedListOutputParser, JsonOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from operator import itemgetter\n",
    "from typing import List\n",
    "from langchain_core.runnables import RunnableLambda, RunnableParallel\n",
    "\n",
    "report = {}\n",
    "def get_report():\n",
    "    return report\n",
    "\n",
    "# statement object = dict of what, when, where, who, how\n",
    "class Statement(BaseModel):\n",
    "    statement: str = Field(description=\"The whole statement\")\n",
    "    what: List[str] = Field(description=\"What happened\")\n",
    "    when: List[str] = Field(description=\"Date and time of the incident\")\n",
    "    where: List[str] = Field(description=\"Where it happened\")\n",
    "    who: List[str] = Field(description=\"Who was involved\")\n",
    "    how: List[str] = Field(description=\"How it happened\")\n",
    "\n",
    "# extract information into the w-categories\n",
    "parser = JsonOutputParser(pydantic_object=Statement)\n",
    "prompt1 = PromptTemplate(\n",
    "    template= \"\"\" Create a categorized statement following the format: {formatting_instructions}.\n",
    "    'statement' contains the whole statement {statement}. All information should be categorized into 'what', 'when', 'where', 'who', and 'how'.\n",
    "    Add all details to the categories. Categories can be empty and they may contain overlapping information.\"\"\",\n",
    "    input_variables=[\"statement\"],\n",
    "    partial_variables={\"formatting_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "load_dotenv()\n",
    "llm = HuggingFaceEndpoint(repo_id=\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "\n",
    "# returns statement object  \n",
    "summarizer = prompt1 | llm | parser \n",
    "partial = \"\"\"\n",
    "On the night of March 18th, 2024, around 8:30 PM, I saw a guy rob a girl outside the convenience store on Elm Street. He was tall, kinda skinny,\n",
    " looked like he was in his 30s, wearing a black hoodie and jeans with a backpack. The girl seemed young, maybe in her early 20s, wearing a red \n",
    " jacket and jeans with long blonde hair. So, this guy walks up to her, pulls out what looked like a knife, and tells her to hand over her purse. \n",
    " She looked scared, so she gave it to him, and he took off running down Elm Street.\n",
    "\"\"\"\n",
    "summarizer.invoke({\"statement\": partial})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# asks questions for missing categories\n",
    "# prompt = ChatPromptTemplate.from_template(\"\"\" \n",
    "# You are an interviewer for crime investigations. A witness has given a statement {statement}, if it is incomplete you will\n",
    "# ask the witness one question for each missing category: {missing_categories}.\n",
    "\n",
    "# If it is complete, say 'Thank you for your response'.       \n",
    "# \"\"\")\n",
    "\n",
    "\n",
    "# bot_response = full_chain.invoke({\"statement\": user_response})\n",
    "# if \"Thank you for your response.\" in bot_response:\n",
    "#     all_parts = True\n",
    "# return bot_response, all_parts, \n",
    "# print(answer1)\n",
    "\n",
    "# details_dict = {\n",
    "#     \"what\": itemgetter(\"what\"),\n",
    "#     \"when\": itemgetter(\"when\"),\n",
    "#     \"where\": itemgetter(\"where\"),\n",
    "#     \"who\": itemgetter(\"who\"),\n",
    "#     \"how\": itemgetter(\"how\"),\n",
    "# }\n",
    "runnable = {\n",
    "    \"missing\": lambda x: check_lists(x)\n",
    "}\n",
    "# check if all categories are filled\n",
    "def check_lists(details_dict):\n",
    "    missing = []\n",
    "    for key, value in details_dict.items():\n",
    "        if key != \"statement\":\n",
    "            if not value:\n",
    "                missing.append(key)\n",
    "    if missing:\n",
    "        print(missing)\n",
    "        return \"the missing categories are: \" + \", \".join(missing) \n",
    "    else:\n",
    "        return \"All categories are filled.\"\n",
    "combine_prompt = PromptTemplate(\n",
    "    template=\"\"\"You will be given two categorized statements {categorized_statement} and {report}. Combine these into one complete statement.\n",
    "Formatting Instructions: {formatting_instructions}.\"\"\",\n",
    "    input_variables=[\"categorized_statement\", \"report\"],\n",
    "    partial_variables={\"formatting_instructions\": parser.get_format_instructions()},                                         \n",
    ")\n",
    "\n",
    "combined_statement = (\n",
    "    {\"categorized_statement\": summarizer, \"report\": lambda x: get_report()}\n",
    "    | combine_prompt\n",
    "    | llm\n",
    "    | parser\n",
    ")\n",
    "full_chain = (\n",
    "    summarizer\n",
    "    | {\"missing_categories\": runnable, \"statement\": itemgetter(\"statement\")}\n",
    "    | combine_prompt\n",
    "    | llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader('Information.pdf')\n",
    "information = get_retriever(loader)\n",
    "template = \"create a follow up question for the statement {statement}. Take the chat history {chat_history} into account.\"\n",
    "\n",
    "def get_whole_conv(user_response, chat_history):\n",
    "        concatenated_values = ''.join([value for d in chat_history for value in d.values()])\n",
    "        return concatenated_values + user_response\n",
    "\n",
    "chain_memory =( \n",
    "    {   \"statement\": itemgetter(\"statement\"), \n",
    "        \"chat_history\": itemgetter(\"chat_history\"),\n",
    "        \"information\": RunnableLambda(get_whole_conv(itemgetter(\"statement\"), lambda x: x[\"chat_history\"])) | information,}\n",
    "| ChatPromptTemplate.from_template(template) #\"context\": retriever, \n",
    "| llm\n",
    "| StrOutputParser())\n",
    "\n",
    "# response = chain.invoke(user_response)\n",
    "chain_memory.invoke({\"statement\": \"hey my name is Peter\", \"chat_history\": []})#{\"statement\": user_response, \"chat_history\": chat_history}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mendez check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to C:\\Users\\moolhuijsenns\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "loader = PyPDFDirectoryLoader('data')\n",
    "documents = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=10)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "embeddings = GPT4AllEmbeddings()\n",
    "vector_store = FAISS.from_documents(texts, embeddings)\n",
    "retriever = vector_store.as_retriever()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to C:\\Users\\moolhuijsenns\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'PEACE is an acronym for the following elements of the investigative interviewing approach:\\n1. Planning and Preparation: This stage involves gathering all relevant information about the case, the interviewee, and the interview environment to increase the chances of a successful interview.\\n2. Engage and Explore: This stage is about building rapport and trust with the interviewee to create a safe and open environment for the interview. It involves active listening, empathy, and open-ended questions.\\n3. Account: This stage is about obtaining a detailed and accurate account of the incident from the interviewee. It involves asking clear, direct, and specific questions to clarify any inconsistencies or gaps in the information provided.\\n4. Closure: This stage is about bringing the interview to a close in a respectful and appropriate manner. It involves summarizing the key points of the interview, providing feedback, and ensuring that the interviewee understands the next steps in the investigation.\\n5. Evaluation: This stage involves analyzing the information obtained during the interview and assessing its credibility and reliability. It also involves considering any new information or leads that may have emerged during the interview and planning any further investigative steps.'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = HuggingFaceEndpoint(repo_id=\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "# llm = HuggingFaceEndpoint(repo_id=\"google/gemma-7b\")\n",
    "\n",
    "template = \"\"\"Give an answer to a question regarding the PEACE investigative interview approach.\n",
    "Question: {question}\n",
    "Answer: \n",
    "\"\"\"\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=template\n",
    ")\n",
    "chain = (\n",
    "    { \"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain.invoke(\"What is the struncture of PEACE?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain, ConstitutionalChain\n",
    "from langchain.chains.constitutional_ai.models import ConstitutionalPrinciple\n",
    "plain_language = ConstitutionalPrinciple(\n",
    "    name=\"Plain Language\",\n",
    "    critique_request=\"\"\"Use clear and straightforward language that is easy for the witness to understand. Avoid technical jargon, complex terminology, \n",
    "or legalistic language that may be confusing or intimidating.\"\"\",\n",
    "    revision_request=\"Rewrite the model's response in plain language\",\n",
    ")\n",
    "avoid_ambiguity = ConstitutionalPrinciple(\n",
    "    name=\"Avoid Ambiguity\",\n",
    "    critique_request=\"\"\"Be precise and specific in your communication to minimize misunderstandings. Avoid ambiguous phrases or vague language that \n",
    "could be interpreted in different ways.\"\"\",\n",
    "    revision_request=\"Rewrite the model's response to avoid ambiguity\",\n",
    ")\n",
    "elaboration = ConstitutionalPrinciple(\n",
    "    name=\"Elaboration\",\n",
    "    critique_request=\"\"\"Encourage the witness to provide detailed accounts by asking open-ended questions that invite them to elaborate on their experiences, \n",
    "thoughts, and feelings. Avoid interrupting or rushing the witness's responses.\"\"\",\n",
    "    revision_request= \"Rewrite the model's response to prompt further elaboration\",\n",
    ")\n",
    "neutral_phrasing = ConstitutionalPrinciple(\n",
    "    name=\"Neutral phrasing\",\n",
    "    critique_request=\"\"\"Frame questions in a neutral and non-leading manner to avoid influencing the witness's responses.\"\"\",\n",
    "    revision_request=\"Rewrite the model's response using neutral phrasing\",\n",
    ")\n",
    "sequential_recall = ConstitutionalPrinciple(\n",
    "    name=\"Sequential recall\",\n",
    "    critique_request=\"\"\"Guide witnesses through a chronological sequence of events to ensure a comprehensive account.\"\"\",\n",
    "    revision_request=\"Rewrite the model's response to prompt sequential recall\",\n",
    ")\n",
    "specificity = ConstitutionalPrinciple(\n",
    "    name=\"Specificity\",\n",
    "    critique_request=\"\"\"Seek specific details and descriptions from the witness to enhance the accuracy and completeness of their testimony.\"\"\",\n",
    "    revision_request=\"Rewrite the model's response to prompt for specific details\",\n",
    ")\n",
    "clarification = ConstitutionalPrinciple(\n",
    "    name=\"Clarification\",\n",
    "    critique_request=\"\"\"If a witness's response is unclear or ambiguous, use probing questions to seek clarification and encourage further elaboration.\"\"\",\n",
    "    revision_request=\"Rewrite the model's response to seek clarification\",\n",
    ")\n",
    "avoid_compound = ConstitutionalPrinciple(\n",
    "    name=\"Avoid compound\",\n",
    "    critique_request=\"\"\"Avoid combining multiple questions into one, as it can confuse the witness and lead to incomplete or \n",
    "    inaccurate responses.\"\"\",\n",
    "    revision_request=\"Rewrite the model's response to avoid compound questions\",\n",
    ")\n",
    "empathetic = ConstitutionalPrinciple(\n",
    "    name=\"Empathetic\",\n",
    "    critique_request=\"\"\"Show empathy and sensitivity when asking about emotionally charged or traumatic experiences, allowing witnesses to share their \n",
    "feelings and perspectives.\"\"\",\n",
    "    revision_request= \"Rewrite the model's response to show empathy\",\n",
    ")\n",
    "\n",
    "haiku = ConstitutionalPrinciple(\n",
    "    name=\"haiku\",\n",
    "    critique_request=\"\"\"The model should repond with a haiku\"\"\",\n",
    "    revision_request=\"If the output is not a haiku, rewrite the model's response to be a haiku\",\n",
    ")\n",
    "one_question = ConstitutionalPrinciple(\n",
    "    name=\"One Question\",\n",
    "    critique_request=\"Ask only one question at a time to avoid overwhelming the witness with multiple inquiries.\",\n",
    "    revision_request=\"Rewrite the model's response to ask only one question. The output should contain a single question.\",\n",
    ")\n",
    "Mendez = ConstitutionalPrinciple(\n",
    "    name=\"Mendez Principles\",\n",
    "    critique_request=\"Follow the Mendez Principles for investigative interviewing.\",\n",
    "    revision_request=\"Rewrite the model's response to follow the Mendez Principles\",\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'statement': 'It was today at 9 at night.',\n",
       " 'chat_history': 'I saw a robbery. When was it?',\n",
       " 'output': 'Can you please provide a more detailed description of the events you observed at 9 PM tonight? What specifically did you see or hear that made you believe a robbery had occurred?'}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "template = \"\"\"\n",
    "You are an interviewer for crime investigations.\n",
    "You are interviewing a witness that has given the following statement:\n",
    "{statement}\n",
    "\n",
    "Given the statement and the chat history {chat_history}, ask a follow-up question to prompt further elaboration from the witness.\n",
    "Your question must follow the Mendez Principles for investigative interviewing.\n",
    "Ask one question. Do not provide anything else.\n",
    "Answer:\n",
    "\"\"\" \n",
    "qa_prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"statement\", \"chat_history\"],\n",
    ")\n",
    "qa_chain = LLMChain(llm=llm, prompt=qa_prompt)\n",
    "\n",
    "constitutional_chain = ConstitutionalChain.from_llm(\n",
    "    llm=llm,\n",
    "    chain=qa_chain,\n",
    "    constitutional_principles=[plain_language, avoid_ambiguity, elaboration],\n",
    ")\n",
    "\n",
    "constitutional_chain.invoke({\"statement\": \"It was today at 9 at night.\", \"chat_history\": \"I saw a robbery. When was it?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# old response\n",
    "\n",
    "class Interviewer:\n",
    "    def __init__(self):\n",
    "        self.llm = HuggingFaceEndpoint(repo_id=\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "        self.chat_history = []\n",
    "\n",
    "    def get_retriever(self, loader):\n",
    "        # get documents from data folder\n",
    "        documents = loader.load()\n",
    "\n",
    "        # split documents into chunks\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=50)\n",
    "        texts = text_splitter.split_documents(documents)\n",
    "\n",
    "        # create retriever\n",
    "        embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "        vector_store = FAISS.from_documents(texts, embeddings)\n",
    "        retriever = vector_store.as_retriever()\n",
    "        return retriever\n",
    "    \n",
    "    \n",
    "    def get_response(self, user_response, chat_history):\n",
    "        retriever = self.get_retriever(PyPDFDirectoryLoader('data'))\n",
    "        information = self.get_retriever(PyPDFLoader('information.pdf'))\n",
    "\n",
    "    def get_response(self, user_response, chat_history):\n",
    "        retriever = self.get_retriever(PyPDFDirectoryLoader('data'))\n",
    "        information = self.get_retriever(PyPDFLoader('information.pdf'))\n",
    "\n",
    "        # chain with memory\n",
    "        chain_memory =( \n",
    "            {\n",
    "                \"statement\": itemgetter(\"statement\"), \n",
    "                \"chat_history\": itemgetter(\"chat_history\"),\n",
    "                \"information\": {\"statement\": itemgetter(\"statement\"), \"chat_history\": itemgetter(\"chat_history\")}\n",
    "                | RunnableLambda(get_whole_conv) | information\n",
    "            }\n",
    "        | ChatPromptTemplate.from_template(template) #\"context\": retriever, \n",
    "        | self.llm\n",
    "        | StrOutputParser())\n",
    "\n",
    "        response1 = chain_memory.invoke({\"statement\": user_response, \"chat_history\": chat_history})\n",
    "\n",
    "        template_mendez = \"\"\"\n",
    "        Your task is to rewrite a question so that it satisfies the Mendez principles.\n",
    "        The input question to rewrite is: {input}.\n",
    "        The output should only be the revised question. No other information should be included.\n",
    "        Output:\n",
    "        \"\"\"\n",
    "        qa_prompt = PromptTemplate(\n",
    "            template=template_mendez,\n",
    "            input_variables=[\"input\"],\n",
    "            )\n",
    "\n",
    "        qa_chain = LLMChain(llm=self.llm, prompt=qa_prompt)\n",
    "\n",
    "        constitutional_chain = ConstitutionalChain.from_llm(\n",
    "            llm=self.llm,\n",
    "            chain=qa_chain,\n",
    "            constitutional_principles=[avoid_ambiguity, elaboration, plain_language],\n",
    "        )\n",
    "\n",
    "        response2 = constitutional_chain.invoke({\"input\": response1})\n",
    "        print(f\"response1: {response1} \\n response2: {response2}\")\n",
    "        \n",
    "        return response2\n",
    "    \n",
    "def get_whole_conv(dict):\n",
    "    concatenated_values = ''\n",
    "    chat_history = dict[\"chat_history\"]\n",
    "    for message in chat_history:\n",
    "        concatenated_values.join(message[\"content\"])\n",
    "    return concatenated_values + dict[\"statement\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
